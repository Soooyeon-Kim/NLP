{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed758d9",
   "metadata": {},
   "source": [
    "## 2) Bag of Words(BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9c538",
   "metadata": {},
   "source": [
    "### 1. Bag of Words란?\n",
    "* Bag of Words를 직역하면 단어들의 가방이라는 의미\n",
    "* 단어들의 순서는 전혀 고려하지 않고, 단어들의 **출현 빈도(frequency)** 에만 집중하는 텍스트 데이터의 수치화 표현 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a86e5c",
   "metadata": {},
   "source": [
    "1. 갖고 있는 어떤 텍스트 문서에 있는 단어들을 가방에다가 전부 넣는다.\n",
    "2. 가방을 흔들어 단어들을 섞는다. \n",
    "     * 만약, 해당 문서 내에서 특정 단어가 N번 등장했다면, 이 가방에는 그 특정 단어가 N개 있게 된다.\n",
    "3. 또한 가방을 흔들어서 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92e8494",
   "metadata": {},
   "source": [
    "#### BoW를 만드는 두 가지 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7dd24",
   "metadata": {},
   "source": [
    "1. 각 단어에 고유한 정수 인덱스를 부여한다.\n",
    "2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다.\n",
    "\n",
    "#### 한국어 예제\n",
    "문서1 : 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\n",
    "\n",
    "문서1에 대한 BoW : \n",
    "입력된 문서1에 대해서 단어 집합(vocaburary)을 만들어 인덱스를 할당하고, BoW를 만드는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83751c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re  \n",
    "okt=Okt()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f5179d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n"
     ]
    }
   ],
   "source": [
    "token=re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")  \n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업\n",
    "token=okt.morphs(token)  \n",
    "\n",
    "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣는다.  \n",
    "\n",
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token:  \n",
    "         if voca not in word2index.keys():  \n",
    "             word2index[voca]=len(word2index)  \n",
    "                \n",
    "# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘긴다.   \n",
    "             bow.insert(len(word2index)-1,1)\n",
    "# BoW 전체에 전부 기본값 1을 넣는다. (단어의 개수는 최소 1개 이상이기 때문)  \n",
    "         else:\n",
    "            index=word2index.get(voca)\n",
    "# 재등장하는 단어의 인덱스를 받아온다.\n",
    "            bow[index]=bow[index]+1\n",
    "\n",
    "# **문서1에 각 단어에 대해서 인덱스를 부여한 결과**\n",
    "# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
    "print(word2index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7684af0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310552ee",
   "metadata": {},
   "source": [
    "* 출력 결과를 보면, 물가상승률의 인덱스는 4이며, 문서1에서 물가상승률은 2번 언급되었기 때문에 인덱스 4(다섯번째 값)에 해당하는 값이 2임을 알 수 있다. (원한다면 한국어에서 불용어에 해당되는 조사들 또한 제거하여 더 정제된 BoW를 만들 수도 있다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155788cf",
   "metadata": {},
   "source": [
    "### 2. Bag of Words의 다른 예제들\n",
    "* BoW에 있어서 중요한 것은 단어의 등장 빈도\n",
    "* 단어의 순서. 즉, 인덱스의 순서는 전혀 상관없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6e5fd",
   "metadata": {},
   "source": [
    "문서1에 대한 인덱스 할당을 임의로 바꾸고 그에 따른 BoW를 만들었을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('발표': 0, '가': 1, '정부': 2, '하는': 3, '소비자': 4, '과': 5, '물가상승률': 6, '느끼는': 7, '은': 8, '다르다': 9)  \n",
    "[1, 2, 1, 1, 1, 1, 2, 1, 1, 1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5cce0",
   "metadata": {},
   "source": [
    "위의 코드에 문서2로 입력으로 하여 인덱스 할당과 BoW를 만드는 것을 진행\n",
    "* 문서2 : 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "140bc6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "token2=re.sub(\"(\\.)\",\"\",\"소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\")  \n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업\n",
    "token2=okt.morphs(token2)  \n",
    "\n",
    "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣는다.  \n",
    "\n",
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token2:  \n",
    "         if voca not in word2index.keys():  \n",
    "             word2index[voca]=len(word2index)  \n",
    "                \n",
    "# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘긴다.   \n",
    "             bow.insert(len(word2index)-1,1)\n",
    "# BoW 전체에 전부 기본값 1을 넣는다. (단어의 개수는 최소 1개 이상이기 때문)  \n",
    "         else:\n",
    "            index=word2index.get(voca)\n",
    "# 재등장하는 단어의 인덱스를 받아온다.\n",
    "            bow[index]=bow[index]+1\n",
    "\n",
    "# **문서1에 각 단어에 대해서 인덱스를 부여한 결과**\n",
    "# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
    "print(word2index)  \n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63654d8",
   "metadata": {},
   "source": [
    "문서1과 문서2를 합쳐서 문서3을 만들고, 입력을 통해 BoW를 만드는 경우\n",
    "* 문서3: 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9979aa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "[1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "token3=re.sub(\"(\\.)\",\"\",\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다. 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\")  \n",
    "# 정규 표현식을 통해 온점을 제거하는 정제 작업\n",
    "token3=okt.morphs(token3)  \n",
    "\n",
    "# OKT 형태소 분석기를 통해 토큰화 작업을 수행한 뒤에, token에다가 넣는다.  \n",
    "\n",
    "word2index={}  \n",
    "bow=[]  \n",
    "for voca in token3:  \n",
    "         if voca not in word2index.keys():  \n",
    "             word2index[voca]=len(word2index)  \n",
    "                \n",
    "# token을 읽으면서, word2index에 없는 (not in) 단어는 새로 추가하고, 이미 있는 단어는 넘긴다.   \n",
    "             bow.insert(len(word2index)-1,1)\n",
    "# BoW 전체에 전부 기본값 1을 넣는다. (단어의 개수는 최소 1개 이상이기 때문)  \n",
    "         else:\n",
    "            index=word2index.get(voca)\n",
    "# 재등장하는 단어의 인덱스를 받아온다.\n",
    "            bow[index]=bow[index]+1\n",
    "\n",
    "# **문서1에 각 단어에 대해서 인덱스를 부여한 결과**\n",
    "# 재등장한 단어는 해당하는 인덱스의 위치에 1을 더해줍니다. (단어의 개수를 세는 것입니다.)  \n",
    "print(word2index)  \n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ab59e",
   "metadata": {},
   "source": [
    "문서3의 단어 집합은 문서1과 문서2의 단어들을 모두 포함하고 있다. \n",
    "* BoW는 종종 여러 문서의 단어 집합을 합친 뒤에, 해당 단어 집합에 대한 각 문서의 BoW를 구하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a305c4",
   "metadata": {},
   "source": [
    "* 문서3에 대한 단어 집합을 기준으로 문서1, 문서2의 BoW를 만드는 경우\n",
    "* 문서3 단어 집합에 대한 문서1 BoW : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
    "* 문서3 단어 집합에 대한 문서2 BoW : [0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 1, 1, 1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9462fa37",
   "metadata": {},
   "source": [
    "문서3 단어 집합에서 물가상승률이라는 단어는 인덱스가 4에 해당된다. 물가상승률이라는 단어는 문서1에서는 2회 등장하며, 문서2에서는 1회 등장하였기 때문에 두 BoW의 인덱스 4의 값은 각각 2와 1이 되는 것을 볼 수 있다.\n",
    "\n",
    "BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이기 때문에, 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰인다.\n",
    "\n",
    "즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰인다.\n",
    "* EX) '달리기', '체력', '근력'과 같은 단어가 자주 등장하면 해당 문서를 체육 관련 문서로 분류할 수 있을 것이며, '미분', '방정식', '부등식'과 같은 단어가 자주 등장한다면 수학 관련 문서로 분류할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb9d2a",
   "metadata": {},
   "source": [
    "### 3. CountVectorizer 클래스로 BoW 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54854a8",
   "metadata": {},
   "source": [
    "사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원\n",
    "* 이를 이용하면 영어에 대해서는 손쉽게 BoW를 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e50fbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28d691b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0bc77",
   "metadata": {},
   "source": [
    "예제 문장에서 **you**와 **love**는 두 번씩 언급되었으므로 각각 **인덱스 2와 인덱스 4**에서 **2의 값**을 가지며, 그 외의 값에서는 1의 값을 가지는 것을 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92dd580",
   "metadata": {},
   "source": [
    "또한 알파벳 I는 BoW를 만드는 과정에서 사라졌는데, \n",
    "이는 CountVectorizer가 기본적으로 **길이가 2이상인 문자에 대해서만** 토큰으로 인식하기 때문이다. \n",
    "\n",
    "정제(Cleaning) 챕터에서 언급했듯이, 영어에서는 **길이가 짧은 문자를 제거하는 것 또한 전처리 작업**으로 고려되기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404460a6",
   "metadata": {},
   "source": [
    "#### 주의할 것\n",
    "CountVectorizer는 단지 **띄어쓰기**만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7909d1",
   "metadata": {},
   "source": [
    "* 이는 영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없지만, \n",
    "한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 **제대로 BoW가 만들어지지 않음**을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d675ab",
   "metadata": {},
   "source": [
    "* 예를 들어, 앞서 BoW를 만드는데 사용했던 '정부가 발표하는 **물가상승률과** 소비자가 느끼는 **물가상승률은** 다르다.' 라는 문장을 CountVectorizer를 사용하여 BoW로 만들 경우, CountVectorizer는 **'물가상승률'** 이라는 단어를 인식하지 못 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b9ad1",
   "metadata": {},
   "source": [
    "CountVectorizer는 띄어쓰기를 기준으로 분리한 뒤에 '물가상승률과'와 '물가상승률은' 으로 **조사**를 포함해서 **하나의 단어로 판단**하기 때문에 서로 다른 두 단어로 인식한다. 그리고 '물가상승률과'와 '물가상승률은'이 각자 다른 인덱스에서 1이라는 빈도의 값을 갖게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3617d",
   "metadata": {},
   "source": [
    "**EX2)** I was summer, you were spring\n",
    "You can not change what the seasons bring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d838ffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 2]]\n",
      "{'was': 8, 'summer': 6, 'you': 11, 'were': 9, 'spring': 5, 'can': 1, 'not': 3, 'change': 2, 'what': 10, 'the': 7, 'seasons': 4, 'bring': 0}\n"
     ]
    }
   ],
   "source": [
    "corpus = ['I was summer, you were spring You can not change what the seasons bring']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96203889",
   "metadata": {},
   "source": [
    "### 4. 불용어를 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9dd25",
   "metadata": {},
   "source": [
    "* 불용어 : 자연어 처리에서 별로 의미를 갖지 않는 단어들\n",
    "* BoW를 사용 : 그 문서에서 각 단어가 얼마나 자주 등장했는지를 확인하는 것\n",
    "* 각 단어에 대한 빈도수를 수치화 : 텍스트 내에서 어떤 단어들이 중요한지 나타냄\n",
    "\n",
    "#### BoW를 만들때 불용어를 제거하는 일\n",
    "=> 자연어 처리의 정확도를 높이기 위해 선택할 수 있는 전처리 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8f7f51",
   "metadata": {},
   "source": [
    "영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa611339",
   "metadata": {},
   "source": [
    "#### (1) 사용자가 직접 정의한 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d8f2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ed82eb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]]\n",
      "{'rather': 9, 'be': 2, 'dry': 4, 'but': 3, 'at': 1, 'least': 5, 'alive': 0, 'rain': 8, 'on': 7, 'me': 6}\n"
     ]
    }
   ],
   "source": [
    "text=[\"I'd rather be dry, but at least I'm alive Rain on me\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8612b2c",
   "metadata": {},
   "source": [
    "#### (2) CountVectorizer에서 제공하는 자체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f90461c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0f11117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'dry': 1, 'alive': 0, 'rain': 2}\n"
     ]
    }
   ],
   "source": [
    "text=[\"I'd rather be dry, but at least I'm alive Rain on me\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39017cae",
   "metadata": {},
   "source": [
    "#### (3) NLTK에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67951a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24361107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'rather': 4, 'dry': 1, 'least': 2, 'alive': 0, 'rain': 3}\n"
     ]
    }
   ],
   "source": [
    "text=[\"I'd rather be dry, but at least I'm alive Rain on me\"]\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words =sw)\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
