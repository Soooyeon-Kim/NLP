{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d0ed9c",
   "metadata": {},
   "source": [
    "## 3) N-gram 언어 모델(N-gram Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d108b",
   "metadata": {},
   "source": [
    "* n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종\n",
    "*  다만, 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법을 사용\n",
    "* n-gram에서의 n이 가지는 의미 : 일부 단어를 몇 개 보느냐를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacf1974",
   "metadata": {},
   "source": [
    "### 1. 코퍼스에서 카운트하지 못하는 경우의 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9afc6c",
   "metadata": {},
   "source": [
    "SLM의 한계 : \n",
    "* 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을 수 있음\n",
    "* 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서 그 문장이 존재하지 않을 가능성이 높음 -> 카운트할 수 없을 가능성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e2738",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mi>P</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mtext>is|An adorable little boy</mtext>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>&#x2248;</mo>\n",
    "  <mtext>&#xA0;</mtext>\n",
    "  <mi>P</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mtext>is|boy</mtext>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac899443",
   "metadata": {},
   "source": [
    " An adorable little boy가 나왔을 때 is가 나올 확률을 그냥 boy가 나왔을 때 is가 나올 확률로 생각해본다면?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6afe67",
   "metadata": {},
   "source": [
    "갖고 있는 코퍼스에 An adorable little boy is가 있을 가능성 보다는 boy is라는 더 짧은 단어 시퀀스가 존재할 가능성이 더 높다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b685255",
   "metadata": {},
   "source": [
    " 앞에서는 An adorable little boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 **An adorable little boy가 나온 횟수** 와 **An adorable little boy is가 나온 횟수**를 카운트해야만 했지만, 이제는 단어의 확률을 구하고자 기준 단어의 앞 단어를 전부 포함해서 카운트하는 것이 아니라, 앞 단어 중 **임의의 개수만 포함해서 카운트**하여 *근사*하자는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc663e9",
   "metadata": {},
   "source": [
    "-> 갖고 있는 코퍼스에서 해당 단어의 시퀀스를 카운트할 확률이 높아진다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29afaa",
   "metadata": {},
   "source": [
    "### 2. N-gram\n",
    "* 임의의 개수를 정하기 위한 기준을 위해 사용하는 것\n",
    "* n-gram은 n개의 연속적인 단어 나열을 의미\n",
    "* 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주\n",
    "* n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef09fbc",
   "metadata": {},
   "source": [
    "#### ex)문장 An adorable little boy is spreading smiles이 있을 때,\n",
    "각 n에 대해서 n-gram을 전부 구해보면?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e159d294",
   "metadata": {},
   "source": [
    "* unigrams : an, adorable, little, boy, is, spreading, smiles\n",
    "* bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n",
    "* trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
    "* 4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9babb",
   "metadata": {},
   "source": [
    "n-gram을 사용할 때는 n이 1일 때는 유니그램(unigram), 2일 때는 바이그램(bigram), 3일 때는 트라이그램(trigram)이라고 명명,  4 이상일 때는 gram 앞에 그대로 숫자를 붙여서 명명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf58e25",
   "metadata": {},
   "source": [
    "#### n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존한다\n",
    "'An adorable little boy is spreading' 다음에 나올 단어를 예측하고 싶다고 할 때, n=4라고 한 4-gram을 이용한 언어 모델을 사용한다면, spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd22d414",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>P</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>w</mi>\n",
    "  <mtext>|boy is spreading</mtext>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mfrac>\n",
    "    <mrow>\n",
    "      <mtext>count(boy is spreading</mtext>\n",
    "      <mtext>&#xA0;</mtext>\n",
    "      <mi>w</mi>\n",
    "      <mo stretchy=\"false\">)</mo>\n",
    "    </mrow>\n",
    "    <mtext>count(boy is spreading)</mtext>\n",
    "  </mfrac>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f0ba1",
   "metadata": {},
   "source": [
    "ex ) \n",
    "* 갖고 있는 코퍼스에서 boy is spreading가 1,000번 등장했으며 \n",
    "* boy is spreading insults가 500번, \n",
    "* boy is spreading smiles가 200번 등장했다고 한다면\n",
    "* boy is spreading 다음에 insults가 등장할 확률은 50%이며, \n",
    "* smiles가 등장할 확률은 20%이다. \n",
    "* 확률적 선택에 따라 insults가 더 맞다고 판단함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22de9c2",
   "metadata": {},
   "source": [
    "* <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>P</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mtext>insults|boy is spreading</mtext>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mn>0.500</mn>\n",
    "</math>\n",
    "* <math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>P</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mtext>smiles|boy is spreading</mtext>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mn>0.200</mn>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c35950",
   "metadata": {},
   "source": [
    "### 3. N-gram Language Model의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215d68d",
   "metadata": {},
   "source": [
    "n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생기며 문장을 읽다 보면 앞 부분과 뒷부분의 문맥이 전혀 연결 안 되는 경우도 생길 수 있음\n",
    "#### 전체 문장을 고려한 언어 모델보다는 정확도가 떨어질 수밖에 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec859abe",
   "metadata": {},
   "source": [
    "#### 3 - (1) 희소 문제(Sparsity Problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198be1b",
   "metadata": {},
   "source": [
    "문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ac5d1",
   "metadata": {},
   "source": [
    "#### 3 - (2) n을 선택하는 것은 trade-off 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ad845",
   "metadata": {},
   "source": [
    "* 앞에서 몇 개의 단어를 볼지 n을 정하는 것은 trade-off가 존재 \n",
    "* 임의의 개수인 n을 1보다는 2로 선택하는 것은 거의 대부분의 경우에서 언어 모델의 성능을 높일 수 있음. \n",
    "* ex) spreading만 보는 것보다는 is spreading을 보고 다음 단어를 예측하는 것이 더 정확하기 때문\n",
    "* 이 경우 훈련 데이터가 적절한 데이터였다면 언어 모델이 적어도 spreading 다음에 동사를 고르지 않을 것임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbcb0e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a58c2",
   "metadata": {},
   "source": [
    "#### n을 크게 선택하면 \n",
    "* 실제 훈련 코퍼스에서 해당 n-gram을 카운트할 수 있는 확률은 적어지므로 희소 문제는 점점 심각해진다.\n",
    "* 기본적으로 코퍼스의 모든 n-gram에 대해서 카운트를 해야 하기 때문에 n이 커질수록 모델 사이즈가 커진다는 문제점이 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99451b15",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a19da5",
   "metadata": {},
   "source": [
    "#### n을 작게 선택하면 \n",
    "* 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어짐 \n",
    "\n",
    "\n",
    "=> 그렇기 때문에 적절한 n을 선택해야 하며 앞서 언급한 trade-off 문제로 인해 정확도를 높이려면 **n은 최대 5를 넘게 잡아서는 안 된다**고 권장함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c3241",
   "metadata": {},
   "source": [
    "### 4. 적용 분야(Domain)에 맞는 코퍼스의 수집"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc5be1",
   "metadata": {},
   "source": [
    "어떤 분야인지, 어떤 어플리케이션인지에 따라서 특정 단어들의 확률 분포는 당연히 다르다\n",
    "* 마케팅 분야에서는 마케팅 단어가, 의료 분야에서는 의료 관련 단어가 당연히 빈번하게 등장하는 것과 같이 **언어 모델에 사용하는 코퍼스를 해당 도메인의 코퍼스를 사용**한다면 당연히 언어 모델이 제대로 된 언어 생성을 할 가능성이 높아진다.\n",
    "\n",
    "훈련에 사용된 도메인 코퍼스가 무엇이냐에 따라서 성능이 비약적으로 달라지기 때문에 이를 언어 모델의 약점이라고 하는 경우도 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad922f",
   "metadata": {},
   "source": [
    "### 5. 인공 신경망을 이용한 언어 모델(Neural Network Based Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db242b",
   "metadata": {},
   "source": [
    "N-gram Language Model의 한계점을 극복하기위해 분모, 분자에 숫자를 더해서 카운트했을 때 0이 되는 것을 방지하는 등의 여러 *일반화(generalization)* 방법들이 존재함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19393e9f",
   "metadata": {},
   "source": [
    "하지만 그럼에도 본질적으로 n-gram 언어 모델에 대한 취약점을 완전히 해결하지는 못하였고, 이를 위한 대안으로 N-gram Language Model보다 대체적으로 성능이 우수한 *인공 신경망을 이용한 언어 모델* 이 많이 사용되고 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
