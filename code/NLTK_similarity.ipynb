{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "223243a1",
   "metadata": {},
   "source": [
    "### NLTK(Natural Language Toolkit) \n",
    "텍스트 전처리 및 탐색 코드를 보다 빠르고 간편하게 작성할 수 있게 도와주는 Python 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a37568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:20:16.280151Z",
     "start_time": "2021-11-04T13:20:16.266803Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b511f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:20:26.880132Z",
     "start_time": "2021-11-04T13:20:26.867134Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"i have looked forward to seeing this since i first saw it amoungst her work\",\n",
    "    \"this is a superb movie suitable for all but the very youngest\",\n",
    "    \"i first saw this movie when I was a little kid and fell in love with it at once\",\n",
    "    \"i am sooo tired but the show must go on\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf6309a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:20:35.516396Z",
     "start_time": "2021-11-04T13:20:35.494355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 영어 stopword\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80825713",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:21:00.066297Z",
     "start_time": "2021-11-04T13:21:00.048061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'noone', 'sooo', 'thereafter', 'beyond', 'amoungst', 'among']\n"
     ]
    }
   ],
   "source": [
    "# stopword를 추가하고 업데이트된 stopword 저장\n",
    "new_keywords = ['noone', 'sooo', 'thereafter', 'beyond', 'amoungst', 'among']\n",
    "updated_stopwords = stopwords + new_keywords\n",
    "\n",
    "print(updated_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d6b8ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:32:01.527819Z",
     "start_time": "2021-11-04T13:32:01.507800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['looked', 'forward', 'seeing', 'since', 'first', 'saw', 'work'], ['superb', 'movie', 'suitable', 'youngest'], ['first', 'saw', 'movie', 'I', 'little', 'kid', 'fell', 'love'], ['tired', 'show', 'must', 'go']]\n"
     ]
    }
   ],
   "source": [
    "# stopword로 test_sentences를 전처리하고 tokenized_word에 저장\n",
    "tokenize_list = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    word_list = []\n",
    "    for token in tokens:\n",
    "        if token not in updated_stopwords:\n",
    "            word_list.append(token)\n",
    "    tokenize_list.append(word_list)\n",
    "\n",
    "print(tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097cbf4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:32:54.550511Z",
     "start_time": "2021-11-04T13:32:54.538511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'forward', 'see', 'sinc', 'first', 'saw', 'work']\n"
     ]
    }
   ],
   "source": [
    "# stemming을 해보세요.\n",
    "stemmed_sent = []\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in tokenized_word[0]:\n",
    "    # 첫번째 문장에 대해서만 stemming을 진행 tokenized_word[0]\n",
    "    # 첫번째 문장에 있는 단어들을 하나씩 불러온다. \n",
    "    stemmed_sent.append(stemmer.stem(word))\n",
    "\n",
    "print(stemmed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed5482",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08a565",
   "metadata": {},
   "source": [
    "### 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "test_sentences = [\n",
    "    \"i have looked forward to seeing this since i first saw it amoungst her work\",\n",
    "    \"this is a superb movie suitable for all but the very youngest\",\n",
    "    \"i first saw this movie when I was a little kid and fell in love with it at once\",\n",
    "    \"i am sooo tired but the show must go on\",\n",
    "]\n",
    "\n",
    "# 영어 stopword를 저장 \n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "print(stopwords)\n",
    "\n",
    "# stopword를 추가하고 업데이트된 stopword를 저장 \n",
    "new_keywords = ['noone', 'sooo', 'thereafter', 'beyond', 'amoungst', 'among']\n",
    "updated_stopwords = stopwords + new_keywords\n",
    "\n",
    "print(updated_stopwords)\n",
    "\n",
    "# 업데이트된 stopword로 test_sentences를 전처리하고 tokenized_word에 저장 \n",
    "tokenized_word = []\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    new_sent = []\n",
    "    for token in tokens:\n",
    "        if token not in updated_stopwords:\n",
    "        # 각 문장별로 각기 다른 토큰화된 단어 목록이 생성됨\n",
    "            new_sent.append(token)\n",
    "    tokenized_word.append(new_sent)\n",
    "\n",
    "print(tokenized_word)\n",
    "\n",
    "# stemming을 해보세요.\n",
    "stemmed_sent = []\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in tokenized_word[0]:\n",
    "    # 첫번째 문장에 대해서만 stemming을 진행 tokenized_word[0]\n",
    "    # 첫번째 문장에 있는 단어들을 하나씩 불러온다. \n",
    "    stemmed_sent.append(stemmer.stem(word))\n",
    "\n",
    "print(stemmed_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f426bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:20:03.256155Z",
     "start_time": "2021-11-04T13:20:00.697775Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f519ce",
   "metadata": {},
   "source": [
    "### 문서 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da68e455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:34:26.361488Z",
     "start_time": "2021-11-04T13:34:26.041061Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sooyeon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc31c4e",
   "metadata": {},
   "source": [
    "### sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a53e384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:34:42.078367Z",
     "start_time": "2021-11-04T13:34:42.070365Z"
    }
   },
   "outputs": [],
   "source": [
    "document_1 = 'South Korea is a country in East Asia, constituting the southern part of the Korean Peninsula, and sharing a land border with North Korea. 25 million people, around half of the countrys population of more than 51 million people, live in the Seoul Capital Area, the fifth-largest metropolitan area in the world.'\n",
    "document_2 = 'North Korea is a country in East Asia constituting the northern part of the Korean Peninsula. The country is bordered to the north by China and by Russia along the Amnok and Tumen rivers, and to the south by South Korea, with the heavily fortified Korean Demilitarized Zone (DMZ) separating the two.'\n",
    "\n",
    "word_tokens_document_1 = word_tokenize(document_1)\n",
    "word_tokens_document_2 = word_tokenize(document_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9beef",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "두 집합 $A, B$에서 교집합 $A \\cap B$와 합집합 $A \\cup B$를 구하여 그 둘의 크기를 비교\n",
    "\n",
    "Jaccard similarity $D_1, D_2$ = $\\frac{|D_1 \\cap D_2|}{|D_1 \\cup D_2|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb70a677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:35:13.710230Z",
     "start_time": "2021-11-04T13:35:13.700230Z"
    }
   },
   "outputs": [],
   "source": [
    "# 합집합\n",
    "union_tokens = set(word_tokens_document_1 + word_tokens_document_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45446325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:35:25.115428Z",
     "start_time": "2021-11-04T13:35:25.099432Z"
    }
   },
   "outputs": [],
   "source": [
    "# 교집합\n",
    "intersection_tokens = set(word_tokens_document_1).intersection(set(word_tokens_document_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdff2032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:35:39.434275Z",
     "start_time": "2021-11-04T13:35:39.422272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30158730158730157\n"
     ]
    }
   ],
   "source": [
    "# 자카드 유사도\n",
    "jaccard_similarity = len(intersection_tokens) / len(union_tokens) # 교집합 / 합집합\n",
    "print(jaccard_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e1cc2",
   "metadata": {},
   "source": [
    "문서 내 단어의 개수를 신경쓰지 않는다는 문제점이 존재함<br>\n",
    "단어가 여러 번 사용되었음에도 이를 무시하고 단어의 존재여부만을 가지고 문서 유사도를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b448200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:38:37.808046Z",
     "start_time": "2021-11-04T13:38:37.791901Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3efd6656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:38:49.989440Z",
     "start_time": "2021-11-04T13:38:49.972440Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document. This is very important.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "    'This is Sparta'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b72f0c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T14:12:23.233370Z",
     "start_time": "2021-11-04T14:12:23.201948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'important', 'is', 'one', 'second', 'sparta', 'the', 'third', 'this', 'very']\n",
      "\n",
      "[[0 1 1 1 2 0 0 0 1 0 2 1]\n",
      " [0 2 0 0 1 0 1 0 1 0 1 0]\n",
      " [1 0 0 0 1 1 0 0 1 1 1 0]\n",
      " [0 1 1 0 1 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print()\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c513c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1975d4",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "#### Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce6d82a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T14:13:14.588232Z",
     "start_time": "2021-11-04T14:13:14.581196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first document. This is very important.\n",
      "This document is the second document.\n",
      "And this is the third one.\n",
      "Is this the first document?\n",
      "This is Sparta\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document. This is very important.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "    'This is Sparta'\n",
    "]\n",
    "print(\"\\n\".join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d8aebca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T14:13:29.457947Z",
     "start_time": "2021-11-04T14:13:29.429817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'important', 'is', 'one', 'second', 'sparta', 'the', 'third', 'this', 'very']\n",
      "\n",
      "[[0.         0.29275244 0.35267539 0.43713206 0.41659154 0.\n",
      "  0.         0.         0.24627258 0.         0.41659154 0.43713206]\n",
      " [0.         0.70933829 0.         0.         0.25235002 0.\n",
      "  0.52958485 0.         0.29835887 0.         0.25235002 0.        ]\n",
      " [0.51492278 0.         0.         0.         0.24536346 0.51492278\n",
      "  0.         0.         0.29009851 0.51492278 0.24536346 0.        ]\n",
      " [0.         0.48961805 0.58983706 0.         0.34836727 0.\n",
      "  0.         0.         0.41188214 0.         0.34836727 0.        ]\n",
      " [0.         0.         0.         0.         0.39515588 0.\n",
      "  0.         0.829279   0.         0.         0.39515588 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "corpus = [\n",
    "    'This is the first document. This is very important.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "    'This is Sparta'\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print()\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd4146ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T14:13:51.012277Z",
     "start_time": "2021-11-04T14:13:51.006272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between first and second documents: [0.49139188]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim = cosine_similarity(X[0], X[1]).flatten()\n",
    "print(\"Similarity between first and second documents: {}\".format(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc399f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
