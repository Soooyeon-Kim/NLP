{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ac3629",
   "metadata": {},
   "source": [
    "## 토큰화 & 형태소 분석\n",
    "토큰화 (Tokenizing) : 글, 문단, 문장 따위를 그 하위 요소(문단, 문장, 단어)로 분절하는 것 \n",
    "\n",
    "형태소 분석 (morpheme analysis) : 이러한 토큰화 중 문장을 의미가 있는 요소 별로 분절하여 이를 분석하는 것 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306cfc28",
   "metadata": {},
   "source": [
    "### 문장의 토큰화와 형태소 분석을 위해 필요한 함수/라이브러리\n",
    "\n",
    "* from konlpy.tag import Okt\n",
    ": 한국어 자연어 처리를 위한 konlpy 라이브러리 Okt 모듈 <br>\n",
    "Okt 모듈은 konlpy에 속한 여러 형태소 분석기 중 하나 <br><br>\n",
    "\n",
    "* Okt.morphs(sentence)\n",
    ": 한국어 문장인 sentence를 형태소에 따라 분할 <br><br>\n",
    "\n",
    "* Okt.pos(sentence)\n",
    ": 한국어 문장인 sentence를 형태소와 그에 따른 품사에 따라 분할 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e52b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T03:13:15.669863Z",
     "start_time": "2021-11-08T03:13:15.203658Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76dc74aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T03:13:16.971155Z",
     "start_time": "2021-11-08T03:13:16.948133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. \\'문서\\'를 \\'문단\\'의 리스트로 변환하는 함수 doc2para를 완성합니다.\\n\\n   Step01. 문장의 마침표(.) 뒤에 있는 개행 표시(\\n)를 기준으로 \\n           문서 내 글들을 리스트 요소 즉, 문단으로 나눕니다.\\n           \\n   Step02. 나누어진 글들 중 마지막 글자가 \".\"인 경우만 \\n           문단이 나누어진 것으로 보고 그 외의 문장들은 서로 \\n           병합하여 줍니다.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8586017",
   "metadata": {},
   "source": [
    "### doc2para\n",
    "'문서'를 '문단'의 리스트로 변환하는 함수\n",
    "* 문장의 마침표(.) 뒤에 있는 개행 표시(\\n)를 기준으로 문서 내 글들을 리스트 요소 즉, 문단으로 나눈다.\n",
    "* 나누어진 글들 중 마지막 글자가 \".\"인 경우만 문단이 나누어진 것으로 보고 그 외의 문장들은 서로 병합한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "820b2662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T03:13:26.112551Z",
     "start_time": "2021-11-08T03:13:26.094548Z"
    }
   },
   "outputs": [],
   "source": [
    "def doc2para(writing):\n",
    "    \n",
    "    paragraphs = []\n",
    "    # 개행문자를 구분해서 처리\n",
    "    splited = writing.split('\\n')\n",
    "    # 리스트의 길이가 0보다 큰 값 (공백인 경우 제거)\n",
    "    splited = list(filter(lambda x: len(x) > 0 , splited))\n",
    "    para = \"\"\n",
    "    \n",
    "    for sentence in splited:\n",
    "        if sentence[-1] != '.':\n",
    "            para += sentence\n",
    "        else:\n",
    "            paragraphs.append(para)\n",
    "            # 아래 코드를 추가하여 문단 내 마지막 문장을 읽어올 수 있습니다.\n",
    "            para += sentence\n",
    "    \n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a81ffe1",
   "metadata": {},
   "source": [
    "### para2sen\n",
    "'문단'을 '문장'의 리스트로 변환하는 함수\n",
    "* 문단을 \".\"으로 나누어 리스트로 만들고, 변수 sentences에 저장\n",
    "* sentences 내 문장들에 대해서 \"?\"로 재분할 한 후,<br> ndarray.flatten()을 활용하여 재분할된 문장이 합쳐질 수 있도록 리스트로 만든다. <br> (\"!\"에 대해서도 마찬가지로 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26f645c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T03:13:48.996230Z",
     "start_time": "2021-11-08T03:13:48.987236Z"
    }
   },
   "outputs": [],
   "source": [
    "def para2sen(paragraph):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    # Step01.\n",
    "    sentences = paragraph.split('.')\n",
    "    \n",
    "    # Step02. \n",
    "    sentences = [sentence.split('?') for sentence in sentences]\n",
    "    sentences = np.array(sentences).flatten()\n",
    "    sentences = [sentence.split('!') for sentence in sentences]\n",
    "    sentences = np.array(sentences).flatten()\n",
    "    sentences = [ sentence.replace('\"','') for sentence in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e064832",
   "metadata": {},
   "source": [
    "### sen2words_byspace\n",
    "띄어쓰기로 문장을 구분하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5b84496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T03:13:57.745803Z",
     "start_time": "2021-11-08T03:13:57.729708Z"
    }
   },
   "outputs": [],
   "source": [
    "def sen2words_byspace(sentence):\n",
    "    \n",
    "    words = []\n",
    "    words = sentence.strip().split(\" \")\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848326c9",
   "metadata": {},
   "source": [
    "### sen2morph\n",
    "'Okt()'로 선언된 Tokenizer인 'analyzer'를 이용해 형태소에 따라 분할된 문장의 리스트를 변수 'morphs'에 저장하는 sen2morph\n",
    "    함수 \n",
    "* Okt.morphs 메소드를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c74d96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T03:14:04.656397Z",
     "start_time": "2021-11-08T03:14:04.650101Z"
    }
   },
   "outputs": [],
   "source": [
    "def sen2morph(sentence):\n",
    "    \n",
    "    morphs = []\n",
    "    \n",
    "    analyzer = Okt()\n",
    "    morphs = analyzer.morphs(sentence)\n",
    "    \n",
    "    return morphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ac126",
   "metadata": {},
   "source": [
    "### analyzing_morphs\n",
    "'Okt()'로 선언된 Tokenizer인 'analyzer'를 이용해 형태소와 그에 따른 품사를 분할하는 analyzing_morphs 함수\n",
    "* Okt.pos 메소드를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0321e3ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T03:14:12.555855Z",
     "start_time": "2021-11-08T03:14:12.551146Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyzing_morphs(sentence):\n",
    "    \n",
    "    analyzer = Okt()\n",
    "    \n",
    "    return analyzer.pos(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4122fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-08T03:14:48.715700Z",
     "start_time": "2021-11-08T03:14:43.694419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장으로 구분된 5번째 문단:  ['\\ufeff혈(血)의 누(淚)일청전쟁(日淸戰爭)의 총소리는 평양 일경이 떠나가는 듯하더니, 그 총소리가 그치매 사람의 자취는 끊어지고 산과 들에 비린 티끌뿐이라', '평양성의 모란봉에 떨어지는 저녁 볕은 뉘엿뉘엿 넘어가는데, 저 햇빛을 붙들어매고 싶은 마음에 붙들어매지는 못하고 숨이 턱에 닿은 듯이 갈팡질팡하는 한 부인이 나이 삼십이 될락말락하고, 얼굴은 분을 따고 넣은 듯이 흰 얼굴이나 인정 없이 뜨겁게 내리쪼이는 가을 볕에 얼굴이 익어서 선앵둣빛이 되고, 걸음걸이는 허둥지둥하는데 옷은 흘러내려서 젖가슴이 다 드러나고 치맛자락은 땅에 질질 끌려서 걸음을 걷는 대로 치마가 밟히니, 그 부인은 아무리 급한 걸음걸이를하더라도 멀리 가지도 못하고 허둥거리기만 한다', '남이 그 모양을 볼 지경이면 저렇게 어여쁜 젊은 여편네가 술 먹고 한길에 나와서 주정한다 할터이나, 그 부인은 술 먹었다 하는 말은 고사하고 미쳤다, 지랄한다 하더라도 그따위 소리는 귀에 들리지 아니할 만하더라', '무슨 소회가 그리 대단한지 그 부인더러 물을 지경이면 대답할 여가도 없이 옥련이를 부르면서돌아다니더라', '“옥련아, 옥련아 옥련아 옥련아, 죽었느냐 살았느냐', ' 죽었거든 죽은 얼굴이라도 한번 다시 만나보자', ' 옥련아 옥련아, 살았거든 어미 애를 그만 쓰이고 어서 바삐 내 눈에 보이게 하여라', ' 옥련아, 총에 맞아 죽었느냐, 창에 찔려 죽었느냐, 사람에게 밟혀 죽었느냐', ' 어리고 고운 살에 가시가 박힌 것을 보아도 어미 된 이내 마음에 내 살이 지겹게 아프던 내 마음이라', ' 오늘 아침에 집에서 떠나올 때에 옥련이가 내 앞에 서서 아장아장 걸어다니면서, 어머니 어서 갑시다 하던 옥련이가 어디로 갔느냐', '”하면서 옥련이를 찾으려고 골몰한 정신에, 옥련이보다 열 갑절 스무 갑절 더 소중하게 생각하는사람을 잃고도 모르고 옥련이만 부르며 다니다가 목이 쉬고 기운이 탈진하여 산비탈 잔디풀 위에 털썩 주저앉았다가 혼자말로 옥련 아버지는 옥련이 찾으려고 저 건너 산 밑으로 가더니 어디']\n",
      "\n",
      "띄어쓰기로 구분된 문장 (5번째 문단의 4번째 문장):  ['무슨', '소회가', '그리', '대단한지', '그', '부인더러', '물을', '지경이면', '대답할', '여가도', '없이', '옥련이를', '부르면서돌아다니더라']\n",
      "\n",
      "형태소 별로 구분된 문장 (5번째 문단의 4번째 문장):  ['무슨', '소', '회', '가', '그리', '대단한지', '그', '부인', '더러', '물', '을', '지경', '이면', '대답', '할', '여가', '도', '없이', '옥련', '이를', '부르면서', '돌아다니더라']\n",
      "\n",
      "형태소와 그에 따른 품사로 분류된 문장 (5번째 문단의 4번째 문장):  [('무슨', 'Noun'), ('소', 'Modifier'), ('회', 'Noun'), ('가', 'Josa'), ('그리', 'Verb'), ('대단한지', 'Adjective'), ('그', 'Noun'), ('부인', 'Noun'), ('더러', 'Josa'), ('물', 'Noun'), ('을', 'Josa'), ('지경', 'Noun'), ('이면', 'Noun'), ('대답', 'Noun'), ('할', 'Verb'), ('여가', 'Noun'), ('도', 'Josa'), ('없이', 'Adverb'), ('옥련', 'Noun'), ('이를', 'Josa'), ('부르면서', 'Verb'), ('돌아다니더라', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    DATA_PATH = \"blood_rain.txt\"\n",
    "    \n",
    "    blood_rain = load_data(DATA_PATH)\n",
    "    paragraphs = doc2para(blood_rain)\n",
    "    sentences = para2sen(paragraphs[4])\n",
    "    words_byspace = sen2words_byspace(sentences[3])\n",
    "    words_bymorphs = sen2morph(sentences[3])\n",
    "    morphs_analyzed = analyzing_morphs(sentences[3])\n",
    "    \n",
    "    # 출력을 통해 토큰화가 잘 되었는지 확인합니다.\n",
    "    \n",
    "    print(\"문장으로 구분된 5번째 문단: \", sentences)\n",
    "    print(\"\\n띄어쓰기로 구분된 문장 (5번째 문단의 4번째 문장): \", words_byspace)\n",
    "    print(\"\\n형태소 별로 구분된 문장 (5번째 문단의 4번째 문장): \", words_bymorphs)\n",
    "    print(\"\\n형태소와 그에 따른 품사로 분류된 문장 (5번째 문단의 4번째 문장): \", morphs_analyzed)\n",
    "    \n",
    "    return words_byspace, words_bymorphs, morphs_analyzed\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1cc3e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949443b1",
   "metadata": {},
   "source": [
    "### 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38a0f1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-08T02:50:20.265Z"
    }
   },
   "outputs": [],
   "source": [
    "def doc2para(writing):\n",
    "    \n",
    "    paragraphs = []\n",
    "    \n",
    "    splited = writing.split('\\n')\n",
    "    # 리스트의 길이가 0보다 큰 값 (공백인 경우 제거)\n",
    "    splited = list(filter(lambda x: len(x) > 0 , splited))\n",
    "    para = \"\"\n",
    "    \n",
    "    for sentence in splited:\n",
    "        if sentence[-1] != '.':\n",
    "            para += sentence\n",
    "        else:\n",
    "            paragraphs.append(para)\n",
    "            # 아래 코드를 추가하여 문단 내 마지막 문장을 읽어올 수 있다.\n",
    "            para += sentence\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def para2sen(paragraph):\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    # 1.split\n",
    "    sentences = paragraph.split('.')\n",
    "    \n",
    "    # 2.split, replace\n",
    "    sentences = [sentence.split('?') for sentence in sentences]\n",
    "    sentences = np.array(sentences).flatten()\n",
    "    sentences = [sentence.split('!') for sentence in sentences]\n",
    "    sentences = np.array(sentences).flatten()\n",
    "    sentences = [ sentence.replace('\"','') for sentence in sentences]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "def sen2words_byspace(sentence):\n",
    "    \n",
    "    words = []\n",
    "    words = sentence.strip().split(\" \")\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def sen2morph(sentence):\n",
    "    \n",
    "    morphs = []\n",
    "    \n",
    "    analyzer = Okt()\n",
    "    morphs = analyzer.morphs(sentence)\n",
    "    \n",
    "    return morphs\n",
    "\n",
    " \n",
    "\n",
    "def analyzing_morphs(sentence):\n",
    "    \n",
    "    analyzer = Okt()\n",
    "    \n",
    "    return analyzer.pos(sentence)\n",
    "    \n",
    "# 위에서 정의한 함수들을 바탕으로 문서를 토큰화를 진행합니다.\n",
    "\n",
    "def main():\n",
    "    \n",
    "    DATA_PATH = \"blood_rain.txt\"\n",
    "    \n",
    "    blood_rain = load_data(DATA_PATH)\n",
    "    paragraphs = doc2para(blood_rain)\n",
    "    sentences = para2sen(paragraphs[4])\n",
    "    words_byspace = sen2words_byspace(sentences[3])\n",
    "    words_bymorphs = sen2morph(sentences[3])\n",
    "    morphs_analyzed = analyzing_morphs(sentences[3])\n",
    "    \n",
    "    # 출력을 통해 토큰화가 잘 되었는지 확인합니다.\n",
    "    \n",
    "    print(\"문장으로 구분된 5번째 문단: \", sentences)\n",
    "    print(\"\\n띄어쓰기로 구분된 문장 (5번째 문단의 4번째 문장): \", words_byspace)\n",
    "    print(\"\\n형태소 별로 구분된 문장 (5번째 문단의 4번째 문장): \", words_bymorphs)\n",
    "    print(\"\\n형태소와 그에 따른 품사로 분류된 문장 (5번째 문단의 4번째 문장): \", morphs_analyzed)\n",
    "    \n",
    "    return words_byspace, words_bymorphs, morphs_analyzed\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
